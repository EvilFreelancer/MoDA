{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:58:53.350311Z",
     "start_time": "2024-06-14T15:58:53.339451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open('config.yml'))\n",
    "# config['functions'] = [\n",
    "#     {\"name\": \"finance\",\n",
    "#      \"description\": \"A large language model fine-tuned on earnings call documents. It is specialized in extracting financial KPIs, analyzing financial statements, and providing insights based on financial data.\",\n",
    "#      \"condition\": \"Use this adapter when users ask about finances, need to analyze financial data, discuss financial reports, or seek financial insights and KPIs.\"},\n",
    "#     {\"name\": \"medicine\",\n",
    "#      \"description\": \"A specialized model trained on medical literature and clinical notes. It assists with medical queries, clinical decision-making, and provides information on medical conditions, diagnoses, and treatment options.\",\n",
    "#      \"condition\": \"Use this adapter when users ask about medical information, healthcare advice, clinical decision support, or details about medical conditions and treatments.\"},\n",
    "#     {\"name\": \"leetcode\",\n",
    "#      \"description\": \"A model fine-tuned on competitive programming problems and solutions from LeetCode. This model specializes in providing assistance with coding challenges, algorithm explanations, and data structure problems commonly found on LeetCode.\",\n",
    "#      \"condition\": \"All messages related to this function start with <human>:. Use this adapter when users ask for help with programming, need explanations of LeetCode problems, or seek solutions to algorithm and data structure challenges. It is also suitable for queries related to coding techniques and competitive programming strategies.\"},\n",
    "#     {\"name\": \"exam\",\n",
    "#      \"description\": \"A model designed to assist with exam preparation. It offers explanations, solutions for various academic subjects, practice questions, and study tips.\",\n",
    "#      \"condition\": \"All messages related to this function start with <human>:. Use this adapter when users need help with exam preparation, practice questions, study tips, or explanations for academic subjects.\"},\n",
    "#     {\"name\": \"webgpt\",\n",
    "#      \"description\": \"A model capable of retrieving and summarizing information from the web in real-time. It is useful for obtaining up-to-date information, facts, and summaries from web searches.\",\n",
    "#      \"condition\": \"Use this adapter when users need up-to-date information, real-time web search capabilities, or summaries of recent events and web data.\"},\n",
    "#     {\"name\": \"gpt4tools\",\n",
    "#      \"description\": \"It generates human-like text and uses tools to indirectly understand images.\",\n",
    "#      \"condition\": \"Users can provide new images to GPT4Tools with a description.\"},\n",
    "#     {\"name\": \"cot\",\n",
    "#      \"description\": \"A model specialized in generating chain-of-thought reasoning. It provides detailed and step-by-step explanations for complex queries, ensuring a logical and thorough response.\",\n",
    "#      \"condition\": \"Use this adapter when users need detailed explanations or step-by-step reasoning for complex queries, involving logical processes and multi-step solutions.\"},\n",
    "#     {\"name\": \"stackoverflow\",\n",
    "#      \"description\": \"A model trained on a wide range of programming and technical questions from StackOverflow. It assists with programming questions, debugging, technical issues, and provides solutions to coding problems.\",\n",
    "#      \"condition\": \"All messages related to this model start with: The conversation between human and AI assistant. Use this adapter when users need help with programming questions, debugging, technical issues, or solutions to coding problems. It is ideal for queries that mirror typical StackOverflow content.\"}\n",
    "# ]\n",
    "\n",
    "config['functions'] = [\n",
    "    {\"name\": \"finance\",\n",
    "     \"description\": \"Is a large language model fine-tuned on earnings call documents to extract financial KPIs from the earnings call documents.\",\n",
    "     \"condition\": \"Use this adapter when users asked you about finances or they need to analyze financial data.\",\n",
    "     # \"keywords\": [\"finance\", \"financial data\", \"earnings\", \"reports\", \"KPIs\", \"investment\", \"stocks\", \"options\"]\n",
    "     },\n",
    "    {\"name\": \"medicine\",\n",
    "     \"description\": \"Is a specialized model trained on medical literature and clinical notes to assist with medical queries and clinical decision-making.\",\n",
    "     \"condition\": \"Use this adapter when users ask about medical information, diagnoses, or treatment options.\"},\n",
    "    {\"name\": \"leetcode\",\n",
    "     \"description\": \"Is a model fine-tuned on competitive programming problems and solutions from LeetCode.\",\n",
    "     \"condition\": \"Use this adapter when users ask for help with programming or need explanations of LeetCode problems, all messages related to this model starts with <human>:\",\n",
    "     \"examples\": [\n",
    "         {\"inputs\": \"<human>: Can you help me solve the two-sum problem on LeetCode?\",\n",
    "          \"name\": \"leetcode\"},\n",
    "         {\"inputs\": \"<human>: What is the best way to implement a binary search algorithm?\",\n",
    "          \"name\": \"leetcode\"},\n",
    "         {\"inputs\": \"<human>: Explain how to find the longest common subsequence in two strings.\",\n",
    "          \"name\": \"leetcode\"},\n",
    "         {\"inputs\": \"<human>: How can I optimize my solution for the 'median of two sorted arrays' problem?\",\n",
    "          \"name\": \"leetcode\"}\n",
    "     ],\n",
    "     },\n",
    "    {\n",
    "        \"name\": \"exam\",\n",
    "        \"description\": \"Is a model designed to assist with exam preparation, offering explanations and solutions for various academic subjects.\",\n",
    "        \"condition\": \"Use this adapter when users need help with exam preparation in school or other study, including practice questions and study tips, all messages related to this model starts with <human>:\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"inputs\": \"<human>: Premise: 'Three camels are carrying passengers through a desert.' Hypothesis: 'Three camels carry bedouins through the desert to an oasis.' Do we know that the hypothesis entailed by the premise? Plus step-by-step reasons.\",\n",
    "                \"name\": \"exam\"},\n",
    "            {\n",
    "                \"inputs\": \"<human>: If 40% of the 880 students at a certain college are enrolled in biology classes, how many students at the college are not enrolled in a biology class? Help me solve this step by step.\",\n",
    "                \"name\": \"exam\"},\n",
    "            {\n",
    "                \"inputs\": \"<human>: If s = { 8, 16, 24, 32, 40, 48 }, what is the product of mean and median of the numbers in s? Let's think step by step.\",\n",
    "                \"name\": \"exam\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"webgpt\",\n",
    "        \"description\": \"Is a model capable of retrieving and summarizing information from the web in real-time.\",\n",
    "        \"condition\": \"Use this adapter when users need up-to-date information or real-time web search capabilities.\",\n",
    "        \"examples\": [\n",
    "            {\"inputs\": \"Voiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?\",\n",
    "             \"name\": \"webgpt\"},\n",
    "            {\"inputs\": \"- The Straw man argument.\\n\\nPlease?\", \"name\": \"webgpt\"},\n",
    "            {\"inputs\": \"Explain: Persistence of Evidence of Civilization\", \"name\": \"webgpt\"},\n",
    "            {\"inputs\": \"In 1965, which Christmas song became the first song to be broadcast from space?\",\n",
    "             \"name\": \"webgpt\"},\n",
    "            {\"inputs\": \"Why is Starcraft so damn popular in Korea?\\n\\nWhy not, say, in China or other countries?\",\n",
    "             \"name\": \"webgpt\"},\n",
    "            {\"inputs\": \"Could you live off breast milk alone?\", \"name\": \"webgpt\"},\n",
    "            {\"inputs\": \"How to start investing\", \"name\": \"webgpt\"}\n",
    "        ],\n",
    "        # \"keywords\": [\n",
    "        #     \"web search\", \"real-time information\", \"news\", \"summaries\",\n",
    "        #     \"recent events\", \"web data\", \"facts\", \"retrieval\", \"up-to-date\"\n",
    "        # ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt4tools\",\n",
    "        \"description\": \"It generates human-like text and uses tools to indirectly understand images.\",\n",
    "        \"condition\": \"Use this adapter when users need comprehensive answers about image that may require the use of multiple external tools.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cot\",\n",
    "        \"description\": \"Is a model specialized in generating chain-of-thought reasoning to provide detailed and step-by-step explanations.\",\n",
    "        \"condition\": \"Use this adapter when users need detailed explanations or step-by-step reasoning for complex queries.\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"inputs\": \"Q: Which of the following sentences is nonsensical?\\nOptions:\\n- Sentence A: \\\"He took his bicycle to the doctor for repair\\\"\\n- Sentence B: \\\"He took his bicycle to the mechanic for repair\\\"\\n\\nLet's solve this gradually.\",\n",
    "                \"name\": \"cot\"\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": \"Imagine a question and stream-of-consciousness explanation for which this is the answer: Sentence B\",\n",
    "                \"name\": \"cot\"\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": \"Student: Given the sentence \\\"A young woman with blond-hair and a pink dior t-shirt checking under the cash drawer.\\\" is it true that \\\"A woman is looking under a drawer.\\\"?.\\nTeacher: Let's think:\",\n",
    "                \"name\": \"cot\"\n",
    "            }\n",
    "        ],\n",
    "        # \"keywords\": [\n",
    "        #     \"chain-of-thought\", \"reasoning\", \"step-by-step explanations\", \"logical processes\",\n",
    "        #     \"multi-step solutions\", \"detailed explanations\"\n",
    "        # ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stackoverflow\",\n",
    "        \"description\": \"Is a model trained on a wide range of programming and technical questions from StackOverflow.\",\n",
    "        \"condition\": \"Use this adapter when users need help with programming questions, debugging, or technical issues.\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"inputs\": \"The conversation between human and AI assistant.\\n[|Human|] \",\n",
    "                \"name\": \"stackoverflow\"\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": \"The conversation between human and AI assistant.\\n[|Human|] I'm interested in learning how to type hint in Python. Can you explain it to me?\\n[|AI|] Sure! Type hints are a way to specify the expected data type of a variable, argument or return value in a function or method. This helps make your code more readable and helps prevent errors during runtime.\\n[|Human|] \",\n",
    "                \"name\": \"stackoverflow\"\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": \"The conversation between human and AI assistant.\\n[|Human|] I'm interested in learning more about regular expression compilers.\\n[|AI|] Sure, I'd be happy to tell you about them. What do you specifically want to know?\\n[|Human|] \",\n",
    "                \"name\": \"stackoverflow\"\n",
    "            },\n",
    "        ],\n",
    "        # \"keywords\": [\n",
    "        #     \"programming\", \"debugging\", \"technical issues\", \"coding problems\", \"StackOverflow\",\n",
    "        #     \"JavaScript\", \"Python\", \"regular expressions\", \"OpenCV\", \"type hints\"\n",
    "        # ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "config"
   ],
   "id": "5f11fd755bf209ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'router': {'name': 'hiieu/Meta-Llama-3-8B-Instruct-function-calling-json-mode',\n",
       "  'load_in_8bit': True},\n",
       " 'socket': {'name': 'meta-llama/Llama-2-7b-hf', 'load_in_8bit': True},\n",
       " 'functions': [{'name': 'finance',\n",
       "   'description': 'Is a large language model fine-tuned on earnings call documents to extract financial KPIs from the earnings call documents.',\n",
       "   'condition': 'Use this adapter when users asked you about finances or they need to analyze financial data.'},\n",
       "  {'name': 'medicine',\n",
       "   'description': 'Is a specialized model trained on medical literature and clinical notes to assist with medical queries and clinical decision-making.',\n",
       "   'condition': 'Use this adapter when users ask about medical information, diagnoses, or treatment options.'},\n",
       "  {'name': 'leetcode',\n",
       "   'description': 'Is a model fine-tuned on competitive programming problems and solutions from LeetCode.',\n",
       "   'condition': 'Use this adapter when users ask for help with programming or need explanations of LeetCode problems, all messages related to this model starts with <human>:',\n",
       "   'examples': [{'inputs': '<human>: Can you help me solve the two-sum problem on LeetCode?',\n",
       "     'name': 'leetcode'},\n",
       "    {'inputs': '<human>: What is the best way to implement a binary search algorithm?',\n",
       "     'name': 'leetcode'},\n",
       "    {'inputs': '<human>: Explain how to find the longest common subsequence in two strings.',\n",
       "     'name': 'leetcode'},\n",
       "    {'inputs': \"<human>: How can I optimize my solution for the 'median of two sorted arrays' problem?\",\n",
       "     'name': 'leetcode'}]},\n",
       "  {'name': 'exam',\n",
       "   'description': 'Is a model designed to assist with exam preparation, offering explanations and solutions for various academic subjects.',\n",
       "   'condition': 'Use this adapter when users need help with exam preparation in school or other study, including practice questions and study tips, all messages related to this model starts with <human>:',\n",
       "   'examples': [{'inputs': \"<human>: Premise: 'Three camels are carrying passengers through a desert.' Hypothesis: 'Three camels carry bedouins through the desert to an oasis.' Do we know that the hypothesis entailed by the premise? Plus step-by-step reasons.\",\n",
       "     'name': 'exam'},\n",
       "    {'inputs': '<human>: If 40% of the 880 students at a certain college are enrolled in biology classes, how many students at the college are not enrolled in a biology class? Help me solve this step by step.',\n",
       "     'name': 'exam'},\n",
       "    {'inputs': \"<human>: If s = { 8, 16, 24, 32, 40, 48 }, what is the product of mean and median of the numbers in s? Let's think step by step.\",\n",
       "     'name': 'exam'}]},\n",
       "  {'name': 'webgpt',\n",
       "   'description': 'Is a model capable of retrieving and summarizing information from the web in real-time.',\n",
       "   'condition': 'Use this adapter when users need up-to-date information or real-time web search capabilities.',\n",
       "   'examples': [{'inputs': 'Voiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?',\n",
       "     'name': 'webgpt'},\n",
       "    {'inputs': '- The Straw man argument.\\n\\nPlease?', 'name': 'webgpt'},\n",
       "    {'inputs': 'Explain: Persistence of Evidence of Civilization',\n",
       "     'name': 'webgpt'},\n",
       "    {'inputs': 'In 1965, which Christmas song became the first song to be broadcast from space?',\n",
       "     'name': 'webgpt'},\n",
       "    {'inputs': 'Why is Starcraft so damn popular in Korea?\\n\\nWhy not, say, in China or other countries?',\n",
       "     'name': 'webgpt'},\n",
       "    {'inputs': 'Could you live off breast milk alone?', 'name': 'webgpt'},\n",
       "    {'inputs': 'How to start investing', 'name': 'webgpt'}]},\n",
       "  {'name': 'gpt4tools',\n",
       "   'description': 'It generates human-like text and uses tools to indirectly understand images.',\n",
       "   'condition': 'Use this adapter when users need comprehensive answers about image that may require the use of multiple external tools.'},\n",
       "  {'name': 'cot',\n",
       "   'description': 'Is a model specialized in generating chain-of-thought reasoning to provide detailed and step-by-step explanations.',\n",
       "   'condition': 'Use this adapter when users need detailed explanations or step-by-step reasoning for complex queries.',\n",
       "   'examples': [{'inputs': 'Q: Which of the following sentences is nonsensical?\\nOptions:\\n- Sentence A: \"He took his bicycle to the doctor for repair\"\\n- Sentence B: \"He took his bicycle to the mechanic for repair\"\\n\\nLet\\'s solve this gradually.',\n",
       "     'name': 'cot'},\n",
       "    {'inputs': 'Imagine a question and stream-of-consciousness explanation for which this is the answer: Sentence B',\n",
       "     'name': 'cot'},\n",
       "    {'inputs': 'Student: Given the sentence \"A young woman with blond-hair and a pink dior t-shirt checking under the cash drawer.\" is it true that \"A woman is looking under a drawer.\"?.\\nTeacher: Let\\'s think:',\n",
       "     'name': 'cot'}]},\n",
       "  {'name': 'stackoverflow',\n",
       "   'description': 'Is a model trained on a wide range of programming and technical questions from StackOverflow.',\n",
       "   'condition': 'Use this adapter when users need help with programming questions, debugging, or technical issues.',\n",
       "   'examples': [{'inputs': 'The conversation between human and AI assistant.\\n[|Human|] ',\n",
       "     'name': 'stackoverflow'},\n",
       "    {'inputs': \"The conversation between human and AI assistant.\\n[|Human|] I'm interested in learning how to type hint in Python. Can you explain it to me?\\n[|AI|] Sure! Type hints are a way to specify the expected data type of a variable, argument or return value in a function or method. This helps make your code more readable and helps prevent errors during runtime.\\n[|Human|] \",\n",
       "     'name': 'stackoverflow'},\n",
       "    {'inputs': \"The conversation between human and AI assistant.\\n[|Human|] I'm interested in learning more about regular expression compilers.\\n[|AI|] Sure, I'd be happy to tell you about them. What do you specifically want to know?\\n[|Human|] \",\n",
       "     'name': 'stackoverflow'}]}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:58:56.997970Z",
     "start_time": "2024-06-14T15:58:53.351396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moda.models import load_model\n",
    "\n",
    "router_model, router_tokenizer = load_model(config[\"router\"])\n",
    "router_terminators = [\n",
    "    router_tokenizer.eos_token_id,\n",
    "    router_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ],
   "id": "26c2e03475130853",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46e7150aebec4e37a547a97846b6a321"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmoda\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model\n\u001B[0;32m----> 3\u001B[0m router_model, router_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrouter\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m router_terminators \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      5\u001B[0m     router_tokenizer\u001B[38;5;241m.\u001B[39meos_token_id,\n\u001B[1;32m      6\u001B[0m     router_tokenizer\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<|eot_id|>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m ]\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/moda/models.py:13\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_model\u001B[39m(config):\n\u001B[1;32m      8\u001B[0m     quantization_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(\n\u001B[1;32m      9\u001B[0m         load_in_8bit\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_in_8bit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_in_8bit\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     10\u001B[0m         load_in_4bit\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_in_4bit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_in_4bit\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     11\u001B[0m         llm_int8_enable_fp32_cpu_offload\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     )\n\u001B[0;32m---> 13\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquantization_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m], use_fast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, tokenizer\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    562\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3754\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3744\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3745\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   3747\u001B[0m     (\n\u001B[1;32m   3748\u001B[0m         model,\n\u001B[1;32m   3749\u001B[0m         missing_keys,\n\u001B[1;32m   3750\u001B[0m         unexpected_keys,\n\u001B[1;32m   3751\u001B[0m         mismatched_keys,\n\u001B[1;32m   3752\u001B[0m         offload_index,\n\u001B[1;32m   3753\u001B[0m         error_msgs,\n\u001B[0;32m-> 3754\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3755\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3756\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3757\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# XXX: rename?\u001B[39;49;00m\n\u001B[1;32m   3758\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3760\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3761\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3762\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3764\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3765\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3766\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgguf_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgguf_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3773\u001B[0m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[1;32m   3774\u001B[0m model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4214\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001B[0m\n\u001B[1;32m   4210\u001B[0m                 set_module_tensor_to_device(\n\u001B[1;32m   4211\u001B[0m                     model_to_load, key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m*\u001B[39mparam\u001B[38;5;241m.\u001B[39msize(), dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   4212\u001B[0m                 )\n\u001B[1;32m   4213\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4214\u001B[0m         new_error_msgs, offload_index, state_dict_index \u001B[38;5;241m=\u001B[39m \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4215\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4216\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4217\u001B[0m \u001B[43m            \u001B[49m\u001B[43mloaded_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4218\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstart_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4219\u001B[0m \u001B[43m            \u001B[49m\u001B[43mexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4220\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4221\u001B[0m \u001B[43m            \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4222\u001B[0m \u001B[43m            \u001B[49m\u001B[43moffload_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4223\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate_dict_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4224\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate_dict_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4225\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4226\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4227\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_safetensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4228\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4229\u001B[0m \u001B[43m            \u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4230\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4231\u001B[0m         error_msgs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_error_msgs\n\u001B[1;32m   4232\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:889\u001B[0m, in \u001B[0;36m_load_state_dict_into_meta_model\u001B[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001B[0m\n\u001B[1;32m    887\u001B[0m     set_module_tensor_to_device(model, param_name, param_device, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mset_module_kwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_quantized_param\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001B[39;00m\n\u001B[1;32m    891\u001B[0m     \u001B[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001B[39;00m\n\u001B[1;32m    892\u001B[0m     \u001B[38;5;66;03m# in comparison to the sharded model across GPUs.\u001B[39;00m\n\u001B[1;32m    893\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_enabled() \u001B[38;5;129;01mor\u001B[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:206\u001B[0m, in \u001B[0;36mBnb8BitHfQuantizer.create_quantized_param\u001B[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001B[0m\n\u001B[1;32m    203\u001B[0m         new_value \u001B[38;5;241m=\u001B[39m new_value\u001B[38;5;241m.\u001B[39mT\n\u001B[1;32m    205\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m old_value\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n\u001B[0;32m--> 206\u001B[0m new_value \u001B[38;5;241m=\u001B[39m \u001B[43mbnb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInt8Params\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequires_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_device\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    208\u001B[0m module\u001B[38;5;241m.\u001B[39m_parameters[tensor_name] \u001B[38;5;241m=\u001B[39m new_value\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fp16_statistics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:606\u001B[0m, in \u001B[0;36mInt8Params.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    603\u001B[0m device, dtype, non_blocking, convert_to_format \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39m_parse_to(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m device\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    608\u001B[0m     new_param \u001B[38;5;241m=\u001B[39m Int8Params(\n\u001B[1;32m    609\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mdtype, non_blocking\u001B[38;5;241m=\u001B[39mnon_blocking),\n\u001B[1;32m    610\u001B[0m         requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequires_grad,\n\u001B[1;32m    611\u001B[0m         has_fp16_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_fp16_weights,\n\u001B[1;32m    612\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:579\u001B[0m, in \u001B[0;36mInt8Params.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    576\u001B[0m     \u001B[38;5;66;03m# we store the 8-bit rows-major weight\u001B[39;00m\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;66;03m# we convert this weight to the turning/ampere weight during the first inference pass\u001B[39;00m\n\u001B[1;32m    578\u001B[0m     B \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mhalf()\u001B[38;5;241m.\u001B[39mcuda(device)\n\u001B[0;32m--> 579\u001B[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001B[38;5;241m=\u001B[39m \u001B[43mbnb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdouble_quant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m CBt\n\u001B[1;32m    581\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m SCBt\n",
      "File \u001B[0;32m~/Documents/Repository/gpt/MoDA/venv/lib/python3.11/site-packages/bitsandbytes/functional.py:2524\u001B[0m, in \u001B[0;36mdouble_quant\u001B[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001B[0m\n\u001B[1;32m   2522\u001B[0m     out_col \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(A\u001B[38;5;241m.\u001B[39mshape, device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint8)\n\u001B[1;32m   2523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out_row \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2524\u001B[0m     out_row \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint8\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2526\u001B[0m coo_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2527\u001B[0m ptrA \u001B[38;5;241m=\u001B[39m get_ptr(A)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T15:58:56.998723Z",
     "start_time": "2024-06-14T15:58:56.998665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moda.conversation import Conversation\n",
    "from moda.functions import get_functions_metadata\n",
    "from moda.models import get_input_ids, trigger_model\n",
    "\n",
    "\n",
    "def classifier(messages):\n",
    "    functions_metadata = get_functions_metadata(config['functions'])\n",
    "    conversation = Conversation(functions=functions_metadata)\n",
    "    conversation.add_user_message(messages)\n",
    "    router_input_ids = get_input_ids(router_tokenizer, conversation.messages).to(router_model.device)\n",
    "    router_result = trigger_model(router_input_ids, router_terminators, router_model, router_tokenizer)\n",
    "    return router_result"
   ],
   "id": "70690823ee1a7e53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# id2label = {\n",
    "#     0: \"finance\",\n",
    "#     1: \"medicine\",\n",
    "#     2: \"leetcode\",\n",
    "#     3: \"exam\",\n",
    "#     4: \"webgpt\",\n",
    "#     5: \"gpt4tools\",\n",
    "#     6: \"cot\",\n",
    "#     7: \"stackoverflow\",\n",
    "# }\n",
    "\n",
    "id2label = {\n",
    "    0: \"finance\",\n",
    "    1: \"medicine\",\n",
    "    2: \"cot\",\n",
    "    3: \"stackoverflow\",\n",
    "}\n",
    "\n",
    "eval_file = \"./moa-reproduciton/cache/unified_eval.jsonl\"\n",
    "dataset_eval = load_dataset(\"json\", split=\"train\", data_files=eval_file)\n",
    "labels = dataset_eval.unique(\"labels\")\n",
    "domains = [id2label[label] for label in labels]\n",
    "domains"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from moda.functions import has_function_call, extract_function_call_from_string\n",
    "from datasets import tqdm as hf_tqdm\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Evaluate the classifier on each domain\n",
    "for label in hf_tqdm(labels):\n",
    "    domain = id2label[label]\n",
    "    print(domain)\n",
    "\n",
    "    # if domain in [\"finance\", \"medicine\", \"leetcode\", \"exam\", \"gpt4tools\", \"cot\", \"webgpt\"]:\n",
    "    # if domain in [\"finance\",  \"medicine\", \"cot\"]:\n",
    "    #     continue  # skip passed\n",
    "\n",
    "    domain_dataset = dataset_eval.filter(lambda x: x['labels'] == label)\n",
    "    domain_size = len(domain_dataset)\n",
    "\n",
    "    correct_classifications = 0\n",
    "    i = 0\n",
    "    for record in hf_tqdm(domain_dataset):\n",
    "        # print(record)\n",
    "        # if i % 10 == 0:\n",
    "        #     print(correct_classifications / i * 100)\n",
    "        # if i >= 500:\n",
    "        #     continue\n",
    "        i = i + 1\n",
    "        prediction = classifier(record[\"text\"])\n",
    "        # print(prediction)\n",
    "        function_call = None\n",
    "        if has_function_call(prediction):\n",
    "            function_call = extract_function_call_from_string(prediction)\n",
    "            if function_call:\n",
    "                if function_call['name'].lower() == domain:\n",
    "                    correct_classifications += 1\n",
    "\n",
    "    classifier_accuracy = correct_classifications / domain_size * 100\n",
    "\n",
    "    step = {\n",
    "        \"Domain\": domain.upper(),\n",
    "        \"test size\": domain_size,\n",
    "        \"Router\": f\"{classifier_accuracy:.2f}%\",\n",
    "    }\n",
    "    print(step)\n",
    "\n",
    "    # Add results to the list\n",
    "    results.append(step)\n",
    "\n",
    "# Calculate average\n",
    "average_size = sum([result[\"test size\"] for result in results]) / len(results)\n",
    "average_classifier_accuracy = sum(\n",
    "    [float(result[\"Router\"].strip('%')) for result in results]\n",
    ") / len(results)\n",
    "\n",
    "# Add the average row\n",
    "results.append({\n",
    "    \"Domain\": \"Average\",\n",
    "    \"test size\": average_size,\n",
    "    \"Router\": f\"{average_classifier_accuracy:.2f}%\",\n",
    "})\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"./evaluation_results.csv\", index=False)"
   ],
   "id": "347471c9ee403ec5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
